{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle New House Transactions â€“ Baseline Notebook\n",
        "\n",
        "This notebook implements a first-pass baseline solution for the housing transactions forecasting task. The focus is on clean data preparation, simple feature engineering, a leakage-free time-based validation split, and a single tree-based regressor to create a valid `submission.csv` file ready for Kaggle upload.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying to play it safe, I start by loading the main Python libraries and setting some constants. LightGBM is the model I'm planning to use, and Pandas/Numpy are obviously for the data wrangling part.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "pd.set_option(\"display.max_columns\", 120)\n",
        "pd.set_option(\"display.width\", 150)\n",
        "\n",
        "DATA_DIR = Path(\"/gpfs/helios/home/leonid98/ml-project-2025/data\")\n",
        "TRAIN_DIR = DATA_DIR / \"train\"\n",
        "TARGET_COL = \"amount_new_house_transactions\"\n",
        "SUBMISSION_TARGET_COL = \"new_house_transaction_amount\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next I list all the CSV sources and the columns I plan to keep from each table. It looks long, but it helps me stay consistent when I merge everything later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "MONTHLY_TABLE_KEYS = {\n",
        "    \"new_house\": \"new_house_transactions.csv\",\n",
        "    \"new_house_nearby\": \"new_house_transactions_nearby_sectors.csv\",\n",
        "    \"pre_owned\": \"pre_owned_house_transactions.csv\",\n",
        "    \"pre_owned_nearby\": \"pre_owned_house_transactions_nearby_sectors.csv\",\n",
        "    \"land\": \"land_transactions.csv\",\n",
        "    \"land_nearby\": \"land_transactions_nearby_sectors.csv\",\n",
        "}\n",
        "\n",
        "NEW_HOUSE_FEATURES = [\n",
        "    \"num_new_house_transactions\",\n",
        "    \"area_new_house_transactions\",\n",
        "    \"price_new_house_transactions\",\n",
        "    \"num_new_house_available_for_sale\",\n",
        "    \"area_new_house_available_for_sale\",\n",
        "    \"period_new_house_sell_through\",\n",
        "]\n",
        "\n",
        "NEW_HOUSE_NEARBY_FEATURES = [\n",
        "    \"num_new_house_transactions_nearby_sectors\",\n",
        "    \"area_new_house_transactions_nearby_sectors\",\n",
        "    \"price_new_house_transactions_nearby_sectors\",\n",
        "    \"num_new_house_available_for_sale_nearby_sectors\",\n",
        "    \"area_new_house_available_for_sale_nearby_sectors\",\n",
        "    \"period_new_house_sell_through_nearby_sectors\",\n",
        "]\n",
        "\n",
        "PRE_OWNED_FEATURES = [\n",
        "    \"num_pre_owned_house_transactions\",\n",
        "    \"area_pre_owned_house_transactions\",\n",
        "    \"price_pre_owned_house_transactions\",\n",
        "    \"amount_pre_owned_house_transactions\",\n",
        "]\n",
        "\n",
        "PRE_OWNED_NEARBY_FEATURES = [\n",
        "    \"num_pre_owned_house_transactions_nearby_sectors\",\n",
        "    \"area_pre_owned_house_transactions_nearby_sectors\",\n",
        "    \"price_pre_owned_house_transactions_nearby_sectors\",\n",
        "    \"amount_pre_owned_house_transactions_nearby_sectors\",\n",
        "]\n",
        "\n",
        "LAND_FEATURES = [\n",
        "    \"num_land_transactions\",\n",
        "    \"construction_area\",\n",
        "    \"planned_building_area\",\n",
        "    \"transaction_amount\",\n",
        "]\n",
        "\n",
        "LAND_NEARBY_FEATURES = [\n",
        "    \"num_land_transactions_nearby_sectors\",\n",
        "    \"construction_area_nearby_sectors\",\n",
        "    \"planned_building_area_nearby_sectors\",\n",
        "    \"transaction_amount_nearby_sectors\",\n",
        "]\n",
        "\n",
        "POI_FEATURES = [\n",
        "    \"resident_population\",\n",
        "    \"number_of_shops\",\n",
        "    \"catering\",\n",
        "    \"retail\",\n",
        "    \"transportation_station\",\n",
        "    \"education\",\n",
        "    \"resident_population_dense\",\n",
        "    \"number_of_shops_dense\",\n",
        "]\n",
        "\n",
        "CITY_FEATURES = [\n",
        "    \"gdp_100m\",\n",
        "    \"per_capita_disposable_income_absolute_yuan\",\n",
        "    \"real_estate_development_investment_completed_10k\",\n",
        "]\n",
        "\n",
        "FEATURE_BLOCKS = [\n",
        "    (\"new_house\", NEW_HOUSE_FEATURES),\n",
        "    (\"new_house_nearby\", NEW_HOUSE_NEARBY_FEATURES),\n",
        "    (\"pre_owned\", PRE_OWNED_FEATURES),\n",
        "    (\"pre_owned_nearby\", PRE_OWNED_NEARBY_FEATURES),\n",
        "    (\"land\", LAND_FEATURES),\n",
        "    (\"land_nearby\", LAND_NEARBY_FEATURES),\n",
        "]\n",
        "\n",
        "\n",
        "def normalize_month_string(raw_value: str) -> str:\n",
        "    \"\"\"Convert strings like '2019-Jan' or '2019 Jan' into '2019 Jan'.\"\"\"\n",
        "    if pd.isna(raw_value):\n",
        "        return raw_value\n",
        "    value = str(raw_value).strip()\n",
        "    value = value.replace(\"-\", \" \")\n",
        "    value = \" \".join(value.split())  # collapse duplicate spaces\n",
        "    return value\n",
        "\n",
        "\n",
        "def parse_month_to_datetime(raw_value: str) -> pd.Timestamp:\n",
        "    normalized = normalize_month_string(raw_value)\n",
        "    dt = pd.to_datetime(normalized, format=\"%Y %b\", errors=\"coerce\")\n",
        "    if pd.isna(dt):\n",
        "        dt = pd.to_datetime(normalized, errors=\"coerce\")\n",
        "    return dt\n",
        "\n",
        "\n",
        "def split_id_into_components(id_series: pd.Series) -> pd.DataFrame:\n",
        "    parts = id_series.str.split(\"_\", n=1, expand=True)\n",
        "    parts.columns = [\"month_part\", \"sector\"]\n",
        "    parts[\"month\"] = parts[\"month_part\"].apply(parse_month_to_datetime)\n",
        "    parts[\"sector\"] = parts[\"sector\"].str.strip()\n",
        "    return parts[[\"month\", \"sector\"]]\n",
        "\n",
        "\n",
        "def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n",
        "    denom = np.maximum(np.abs(y_true), eps)\n",
        "    return np.mean(np.abs(y_true - y_pred) / denom)\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    return LGBMRegressor(random_state=42, n_estimators=1000, learning_rate=0.05, subsample=0.9, colsample_bytree=0.8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here I'm writing helper functions for loading and cleaning the raw CSVs. The idea is to parse the dates, tidy sector names, and keep everything in a dictionary so later steps are easier to manage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(data_dir: Path = DATA_DIR) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Read all CSV files required for the baseline pipeline.\"\"\"\n",
        "    train_dir = data_dir / \"train\"\n",
        "    data: Dict[str, pd.DataFrame] = {}\n",
        "    for key, filename in MONTHLY_TABLE_KEYS.items():\n",
        "        data[key] = pd.read_csv(train_dir / filename)\n",
        "    data[\"sector_poi\"] = pd.read_csv(train_dir / \"sector_POI.csv\")\n",
        "    data[\"city_indexes\"] = pd.read_csv(train_dir / \"city_indexes.csv\")\n",
        "    data[\"test\"] = pd.read_csv(data_dir / \"test.csv\")\n",
        "    data[\"sample_submission\"] = pd.read_csv(data_dir / \"sample_submission.csv\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_tables(raw_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"Create cleaned copies with parsed datetime columns and helper keys.\"\"\"\n",
        "    tables: Dict[str, pd.DataFrame] = {}\n",
        "\n",
        "    for key in MONTHLY_TABLE_KEYS:\n",
        "        df = raw_data[key].copy()\n",
        "        df[\"month\"] = df[\"month\"].apply(parse_month_to_datetime)\n",
        "        df[\"sector\"] = df[\"sector\"].str.strip()\n",
        "        tables[key] = df\n",
        "\n",
        "    poi = raw_data[\"sector_poi\"].copy()\n",
        "    poi[\"sector\"] = poi[\"sector\"].str.strip()\n",
        "    tables[\"sector_poi\"] = poi\n",
        "\n",
        "    city_idx = raw_data[\"city_indexes\"].copy()\n",
        "    city_idx[\"year\"] = pd.to_numeric(city_idx[\"city_indicator_data_year\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    tables[\"city_indexes\"] = city_idx\n",
        "\n",
        "    tables[\"test\"] = raw_data[\"test\"].copy()\n",
        "    tables[\"sample_submission\"] = raw_data[\"sample_submission\"].copy()\n",
        "    return tables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This block builds the actual modeling tables: I merge all monthly tables on `(month, sector)`, add POI and city stats, engineer the simple time features, and make sure the target is zero-filled when it's missing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the merging utilities live here: collecting every `(month, sector)` combination, stitching features from each table, converting everything to numbers, and separating train/test with matching columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gather_all_month_sector_pairs(tables: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
        "    frames = []\n",
        "    for key in MONTHLY_TABLE_KEYS:\n",
        "        frames.append(tables[key][[\"month\", \"sector\"]])\n",
        "    combined = pd.concat(frames, ignore_index=True).drop_duplicates()\n",
        "    combined = combined.dropna(subset=[\"month\", \"sector\"]).reset_index(drop=True)\n",
        "    return combined.sort_values([\"month\", \"sector\"]).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def build_feature_frame(base_df: pd.DataFrame, tables: Dict[str, pd.DataFrame], include_target: bool = True) -> pd.DataFrame:\n",
        "    df = base_df.copy()\n",
        "    df = df.dropna(subset=[\"month\", \"sector\"])\n",
        "    df[\"year\"] = df[\"month\"].dt.year.astype(\"Int64\")\n",
        "    df[\"month_num\"] = df[\"month\"].dt.month.astype(\"Int64\")\n",
        "\n",
        "    for table_name, columns in FEATURE_BLOCKS:\n",
        "        available_cols = [col for col in columns if col in tables[table_name].columns]\n",
        "        if not available_cols:\n",
        "            continue\n",
        "        merge_cols = [\"month\", \"sector\"] + available_cols\n",
        "        df = df.merge(tables[table_name][merge_cols], on=[\"month\", \"sector\"], how=\"left\")\n",
        "\n",
        "    poi_cols = [col for col in POI_FEATURES if col in tables[\"sector_poi\"].columns]\n",
        "    if poi_cols:\n",
        "        df = df.merge(tables[\"sector_poi\"][ [\"sector\"] + poi_cols ], on=\"sector\", how=\"left\")\n",
        "\n",
        "    city_cols = [col for col in CITY_FEATURES if col in tables[\"city_indexes\"].columns]\n",
        "    if city_cols:\n",
        "        city_frame = tables[\"city_indexes\"][ [\"year\"] + city_cols ].drop_duplicates(subset=[\"year\"], keep=\"last\")\n",
        "        df = df.merge(city_frame, on=\"year\", how=\"left\")\n",
        "\n",
        "    if include_target:\n",
        "        target_frame = tables[\"new_house\"][ [\"month\", \"sector\", TARGET_COL] ]\n",
        "        df = df.merge(target_frame, on=[\"month\", \"sector\"], how=\"left\")\n",
        "        df[TARGET_COL] = df[TARGET_COL].fillna(0.0)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def numeric_feature_cleanup(df: pd.DataFrame, feature_cols: List[str]) -> pd.DataFrame:\n",
        "    for col in feature_cols:\n",
        "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    df[feature_cols] = df[feature_cols].fillna(0.0)\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_train_data(tables: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    base_pairs = gather_all_month_sector_pairs(tables)\n",
        "    train_df = build_feature_frame(base_pairs, tables, include_target=True)\n",
        "    feature_cols = [col for col in train_df.columns if col not in {\"month\", \"sector\", TARGET_COL}]\n",
        "    train_df = numeric_feature_cleanup(train_df, feature_cols)\n",
        "    train_df = train_df.sort_values([\"month\", \"sector\"]).reset_index(drop=True)\n",
        "    return train_df, feature_cols\n",
        "\n",
        "\n",
        "def prepare_test_data(tables: Dict[str, pd.DataFrame], feature_cols: List[str]) -> pd.DataFrame:\n",
        "    test_df = tables[\"test\"].copy()\n",
        "    month_sector = split_id_into_components(test_df[\"id\"])\n",
        "    test_df = pd.concat([test_df, month_sector], axis=1)\n",
        "    enriched = build_feature_frame(test_df[[\"id\", \"month\", \"sector\"]], tables, include_target=False)\n",
        "\n",
        "    for col in feature_cols:\n",
        "        if col not in enriched.columns:\n",
        "            enriched[col] = 0.0\n",
        "    enriched = numeric_feature_cleanup(enriched, feature_cols)\n",
        "    return enriched[[\"id\", \"month\", \"sector\"] + feature_cols]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For model evaluation I go with a simple rolling-style split: keep the latest ~20% of months as validation, train on the earlier ones, and compute MAE+MAPE after clipping negatives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_validation_months(unique_months: np.ndarray) -> np.ndarray:\n",
        "    total = len(unique_months)\n",
        "    if total <= 1:\n",
        "        return unique_months\n",
        "    val_count = min(6, max(1, total // 5))\n",
        "    if total - val_count < 1:\n",
        "        val_count = max(1, total - 1)\n",
        "    return unique_months[-val_count:]\n",
        "\n",
        "\n",
        "def train_and_validate_model(train_df: pd.DataFrame, feature_cols: List[str]) -> Tuple[object, Dict[str, float]]:\n",
        "    unique_months = train_df[\"month\"].sort_values().unique()\n",
        "    val_months = choose_validation_months(unique_months)\n",
        "\n",
        "    train_mask = ~train_df[\"month\"].isin(val_months)\n",
        "    val_mask = train_df[\"month\"].isin(val_months)\n",
        "\n",
        "    if train_mask.sum() == 0 or val_mask.sum() == 0:\n",
        "        raise ValueError(\"Validation split failed. Please ensure there are enough months of data.\")\n",
        "\n",
        "    X_train = train_df.loc[train_mask, feature_cols]\n",
        "    y_train = train_df.loc[train_mask, TARGET_COL]\n",
        "    X_val = train_df.loc[val_mask, feature_cols]\n",
        "    y_val = train_df.loc[val_mask, TARGET_COL]\n",
        "\n",
        "    model = build_model()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    val_preds = np.clip(model.predict(X_val), 0, None)\n",
        "    metrics = {\n",
        "        \"mae\": mean_absolute_error(y_val, val_preds),\n",
        "        \"mape\": mean_absolute_percentage_error(y_val.values, val_preds),\n",
        "        \"val_months\": val_months,\n",
        "    }\n",
        "    return model, metrics\n",
        "\n",
        "\n",
        "def train_on_full_data_and_predict(train_df: pd.DataFrame, test_df: pd.DataFrame, feature_cols: List[str]) -> Tuple[object, pd.DataFrame]:\n",
        "    model = build_model()\n",
        "    model.fit(train_df[feature_cols], train_df[TARGET_COL])\n",
        "    test_preds = np.clip(model.predict(test_df[feature_cols]), 0, None)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        \"id\": test_df[\"id\"],\n",
        "        SUBMISSION_TARGET_COL: test_preds,\n",
        "    })\n",
        "    submission.to_csv(\"submission.csv\", index=False)\n",
        "    return model, submission\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally I actually run the whole pipeline: load the data, build features, check the validation months, and then fit on all the data to spit out `submission.csv` so I can upload it to Kaggle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training rows: 6,432, features: 41\n",
            "Test rows: 1,152\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 6880\n",
            "[LightGBM] [Info] Number of data points in the train set: 5856, number of used features: 41\n",
            "[LightGBM] [Info] Start training from score 27919.951946\n",
            "Validation months: ['2024-Feb', '2024-Mar', '2024-Apr', '2024-May', '2024-Jun', '2024-Jul']\n",
            "Validation MAE: 1250.68\n",
            "Validation MAPE: 3201167.1658\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001585 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 6883\n",
            "[LightGBM] [Info] Number of data points in the train set: 6432, number of used features: 41\n",
            "[LightGBM] [Info] Start training from score 27526.152219\n",
            "Saved submission.csv with shape: (1152, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>new_house_transaction_amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024 Aug_sector 1</td>\n",
              "      <td>7.516924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024 Aug_sector 2</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024 Aug_sector 3</td>\n",
              "      <td>23.089488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024 Aug_sector 4</td>\n",
              "      <td>24.495626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024 Aug_sector 5</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  new_house_transaction_amount\n",
              "0  2024 Aug_sector 1                      7.516924\n",
              "1  2024 Aug_sector 2                      0.000000\n",
              "2  2024 Aug_sector 3                     23.089488\n",
              "3  2024 Aug_sector 4                     24.495626\n",
              "4  2024 Aug_sector 5                      0.000000"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load every CSV once and normalize shared tables.\n",
        "raw_data = load_data()\n",
        "tables = preprocess_tables(raw_data)\n",
        "\n",
        "# Build the modeling tables with aligned features for train/test.\n",
        "train_df, feature_cols = prepare_train_data(tables)\n",
        "test_df = prepare_test_data(tables, feature_cols)\n",
        "\n",
        "print(f\"Training rows: {len(train_df):,}, features: {len(feature_cols)}\")\n",
        "print(f\"Test rows: {len(test_df):,}\")\n",
        "\n",
        "# Time-aware validation to sanity-check the baseline model.\n",
        "_, val_metrics = train_and_validate_model(train_df, feature_cols)\n",
        "val_month_labels = [pd.to_datetime(m).strftime(\"%Y-%b\") for m in val_metrics[\"val_months\"]]\n",
        "print(\"Validation months:\", val_month_labels)\n",
        "print(f\"Validation MAE: {val_metrics['mae']:.2f}\")\n",
        "print(f\"Validation MAPE: {val_metrics['mape']:.4f}\")\n",
        "\n",
        "# Retrain on the full dataset and export a submission file.\n",
        "full_model, submission = train_on_full_data_and_predict(train_df, test_df, feature_cols)\n",
        "print(\"Saved submission.csv with shape:\", submission.shape)\n",
        "submission.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/competitions/submission-url\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submit -c china-real-estate-demand-prediction -f submission.csv -m \"Baseline submission\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
